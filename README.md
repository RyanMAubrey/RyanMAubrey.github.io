# Title: M.A.V.I.S My Average Very Intelligent System 

## Introduction
Artificial intelligence (AI) is rapidly evolving in today’s age. M.A.V.I.S (My Average Very Intelligent System) is an AI-powered voice assistant designed to provide an engaging and interactive user experience. It is inspired by the fictional AI J.A.R.V.I.S from the Iron Man comics and movies. J.A.R.V.I.S is similar to traditional AI assistants like Siri and Google Assistant, but offers a more conversational and witty personality. Through the integration of speech-to-text, a fine-tuned large language model, waveform audio visualizations and text-to-speech synthesis, M.A.V.I.S offers dynamic and fun interactions through both auditory and visual feedback In contrast to the majority of existing voice assistants that rely on rigid command-based systems, M.A.V.I.S is designed to take into account contextual awareness and user engagement. The model is offered as a mobile app on the IOS platform designed within the XCode editor. By leveraging open-source applications VOSK for speech recognition, RASA for natural language understanding, and Mozilla TTS for speech synthesis, the model balances performance, flexibility, and ethical AI principles. 

M.A.V.I.S is an AI system that not only understands user intent but also communicates in a human-like manner. Our approach to creating M.A.V.I.S uses a sophisticated pipeline that transforms voice input into engaging interactions. It is a complete redesign of the typical AI assistant, from recording audio waveforms to generating smart responses and rendering speech output. We emphasize user interaction, accessibility, and ethics, and have created an AI assistant that has made technology interactions more responsive and engaging. M.A.V.I.S extends its functionality beyond engaging conversations by integrating seamlessly with everyday apps. With direct access to your calendar, it can schedule, reschedule, or remind you of appointments, ensuring you never miss an important date. Additionally, by linking to weather applications, M.A.V.I.S provides real-time weather updates and forecasts, helping you plan your day better. These functionality features are planned as implementation within a minimum viable product.


## Related Works
Research on AI voice assistants covers ethical concerns, speech recognition, NLP, and user interaction. M.A.V.I.S builds on these areas by integrating open-source tools for a more engaging and adaptive experience.

### Ethics and Societal Impact of AI Assistants
Advanced AI assistants are natural language interfaces that can autonomously plan and execute tasks on behalf of users, potentially transforming work, education, and personal interactions. However, Gabriel et al. [1] highlight serious ethical and societal challenges—including issues of value alignment, safety, trust, and misuse—that must be carefully addressed through coordinated technical, policy, and evaluative efforts.

### Speech Recognition and NLP
The Large Language and Speech Model (LLaSM) presented by Shu et al. [2] posits that speech is context-rich and underscores the need for a versatile AI technology that integrates speech and language prompts. It describes the methodology behind LLaSM and its dataset addressing the shortage of open-source speech–text cross-modal instruction-following data. Although tangential to our project, we may diverge in execution by implementing a futuristic user interface or giving our model a specific persona [2]. “My-Assistant,” developed by Gupta et al. [6] focuses on speech recognition and natural language understanding techniques. It details methods for pre-processing, speech signal processing, intent identification, and response generation, discussing system integrations and performance metrics that emphasize enhancing recognition accuracy and intent identification.

### User Interaction and Accessibility
The study done by Yan et al. [3] examines how older people interact with voice assistants, which can differ drastically from interactions by younger users. It proposes modifications to rule-based NLP models with human input to better accommodate the changes in speech patterns that occur with age, aiming to create a more accurate and effective voice assistant for all users. Chatterjee et al. [5] explores an AI-driven system for detecting emotions in speech using techniques such as Mel Frequency Cepstral Coefficients (MFCCs) and convolutional neural networks (CNNs). It also integrates Hidden Markov Models (HMMs) and covariance matrices to refine real-time emotion recognition, which could inspire enhancements in our system’s responsiveness and user engagement.

### AI Chatbots and Voice Interfaces
AI chatbots using GPT-based models and speech-to-text technology enhance voice interactions. The study done by Balamurugan et al. [7] details the design of a speech-controlled AI assistant built with a React-based web application. Using the Web Speech API for speech-to-text conversion and OpenAI’s GPT-3.5 for NLP, the system is evaluated for usability and real-time performance. Insights regarding system constraints and optimization are valuable for refining our own voice assistant implementation.

Unlike existing assistants that rely on predefined commands, M.A.V.I.S combines existing tools to create a more dynamic and context-aware system. By focusing on engagement, personalization, and ethical considerations it aims to make AI interactions more natural and enjoyable.

## Methods
Our project is developed entirely in Python, which serves as the coding language which we wil integrate various open-source tools and libraries. We will begin by using VOSK for speech recognition, which efficiently converts spoken input into text, forming the initial output of our data processing pipeline with a text file (https://alphacephei.com/vosk/). This transcribed text is then fed into our AI assistant built on the RASA open-source framework, where natural language understanding and conversational response generation take place utilizing a deep neural network (https://rasa.com/docs/rasa/). The system further processes these responses by converting the output text back into speech using Mozilla Common Voice TTS to create an audio file (https://commonvoice.mozilla.org/en). Finally, when displayed back through the phone application, the UI will use a waveform generator to create a unique visual (https://github.com/larryleeyu/AudioViz). Our data pipeline also leverages publicly available data sets to train these tools. Mozilla Common Voice TTS is trained with a consistent voice modeled off the LJ Speech dataset (https://keithito.com/LJ-Speech-Dataset/), while VOSK benefits from pre-trained models that require no additional fine-tuning. For the RASA framework, we incorporate a dual-dataset approach using RASA’s sample dataset (https://github.com/RasaHQ/rasa/blob/main/examples/formbot/data/nlu.yml) for intent recognition and the MultiWOZ dataset (https://github.com/budzianowski/multiwoz) for generating realistic dialogue responses on existing examples. Potential pitfalls include challenges such as ambient noise affecting VOSK's accuracy in transcribing user input, latency in processing real-time interactions, and ensuring robust data security and privacy measures throughout the system.
